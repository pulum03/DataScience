{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.1 모델\n",
    "\n",
    "$$y_i = \\beta x_i + \\alpha + \\varepsilon_i$$\n",
    "\n",
    "y_i는 사용자 i가 매일 사이트에서 보내는 시간(분),\n",
    "\n",
    "x_i는 사용자 i의 친구 수.\n",
    "\n",
    "\\varepsilon_i는 모델이 고려하지 못한 다른 요소 때문에 발생하는 (아마도 작은)오류.\n",
    "\n",
    "만약 알파와 베타가 이미 구해졌다면, 다음과 같이 예측 할수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alpha, beta, x_i):\n",
    "    return beta * x_i + alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알파와 베타 어떻게 선택할수 있을까?\n",
    "\n",
    "알파와 베타가 무엇이든 간에 x_i에 대한 결과 예측 가능.\n",
    "\n",
    "실제 출력값이 y_i가 주어졌으니, 다양한 알파와 베타에 대한 오류 계산 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(alpha, beta, x_i, y_i):\n",
    "    # 실제 결과가 y_i일 떄, beta * x_i + alpha로 계산된 예측값의 오류\n",
    "    return y_i - predict(alpha, beta, x_i) # y_i - (beta * x_i + alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 정말 알고 싶은 것은 데잍처 전체에서 발생하는 총 오류값.\n",
    "\n",
    "하지만 무작정 모든 오류값 더하면 x.\n",
    "\n",
    "만약 x_1의 예측값이 너무 높고 x_2의 예측값이 너무 낮다면, 오류값이 상쇄되기 때문.\n",
    "\n",
    "그래서! 오류의 제곱값을 더해줘야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squared_errors(alpha, beta, x, y):\n",
    "    return sum(error(alpha, beta, x_i, y_i)**2\n",
    "               for x_i, y_i in zip(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최소자승법이란 sum_of_squared_errors를 최소화해 주는 알파와 베타 값을 찾는 것을 의미\n",
    "\n",
    "미분을 사용하면 오류를 최소화하는 알파와 베타를 찾을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공식 불러오기\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def de_mean(x):\n",
    "    # x의 모든 데이터 포인트에서 평균을 뺌 (평균을 0으로 만들기 위해)\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "# 벡터 내적\n",
    "def dot(v, w):\n",
    "    # v_1 * w_1 + ... + v_n * w_n\n",
    "    return sum(v_i*w_i for v_i,w_i in zip(v,w))\n",
    "\n",
    "# 내적의 개념을 사용한, 각 성분의 제곱 합\n",
    "def sum_of_squares(v):\n",
    "    # v_1 * v_1 + ... + v_n*v_n\n",
    "    return dot(v,v)\n",
    "\n",
    "def covariance(x,y):\n",
    "    n = len(x)\n",
    "    return dot(de_mean(x), de_mean(y)) / (n-1)\n",
    "\n",
    "def correlation(x,y):\n",
    "    stdev_x = standard_deviation(x)\n",
    "    stdev_y = standard_deviation(y)\n",
    "    \n",
    "    if stdev_x > 0 and stdev_y > 0:\n",
    "        return covariance(x,y) / stdev_x / stdev_y\n",
    "    else:\n",
    "        return 0 # 편차가 존재하지 않는다면 상관관계는 0이다.\n",
    "    \n",
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "\n",
    "def variance(x):\n",
    "    # x에 두 개 이상의 데이터가 있다고 가정\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations) / (n-1)\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    data = list(zip(x, y))\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "\n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(x,y):\n",
    "    # x와 y가 학습 데이터로 주어졌을 때, 오류의 제곱 값을 최소화해 주는 알파와 베타 계산\n",
    "    beta = correlation(x,y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "독립 변수 x의 평균이 주어지면, 알파는 종속 변수 y의 평균을 예측\n",
    "\n",
    "베타는 입력 변수가 standard_deviation(x)만큼 증가한다면 예측값 또한,\n",
    "\n",
    "correlation(x,y) * standard_deviation(y)만큼 증가한다는 것을 의미.\n",
    "\n",
    "만약 x와 y가 완벽한 양의 상관관계를 지닌다면, x가 1 표준편차만큼 증가할 때마다 y 또한 1 표준편차만큼 증가.\n",
    "\n",
    "만약 x와 y가 완벽한 음의 상관관계를 지닌다면, x가 증가할때마다 y는 감소.\n",
    "\n",
    "만약 상관관계가 0이라면 베타는 0이며, 이는 x가 예측에 아무런 영향 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_friends = [100,49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "daily_minutes = [1,68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]\n",
    "outlier = num_friends.index(100)\n",
    "num_friends_good = [x\n",
    "                   for i,x in enumerate(num_friends)\n",
    "                   if i != outlier]\n",
    "\n",
    "daily_minutes_good = [x\n",
    "                     for i,x in enumerate(daily_minutes)\n",
    "                     if i != outlier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.94755241346903\n",
      "0.903865945605865\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = least_squares_fit(num_friends_good, daily_minutes_good)\n",
    "\n",
    "print(alpha)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = [predict(alpha, beta, x_i) for x_i in num_friends_good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(num_friends_good, daily_minutes_good, marker='.', color='red', label='ys1')\n",
    "plt.plot(num_friends_good, y_hat, '-')\n",
    "plt.xlabel('friends')\n",
    "plt.ylabel('minutes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결정계수(R 제곱값)란 종속 변수의 총 변화량 중 모델이 잡아낼 수 있는 변화량의 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3291078377836305"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def total_sum_of_squares(y):\n",
    "    # 평균을 기준으로 y_i의 변화량을 제곱한 값의 총합\n",
    "    return sum(v **2 for v in de_mean(y))\n",
    "\n",
    "def r_squared(alpha, beta, x, y):\n",
    "    # 모델이 잡아낼 수 있는 변화량의 비율은 1 - 모델이 잡아내지 못하는 변화량의 비율로 계산\n",
    "    return 1.0 - (sum_of_squared_errors(alpha, beta, x,y)/\n",
    "                 total_sum_of_squares(y))\n",
    "\n",
    "r_squared(alpha, beta, num_friends_good, daily_minutes_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오류의 제곱값을 최소화하는 알파와 베타 구함.\n",
    "\n",
    "이 밖에 모든 예측값을 mean(y)로 계산해주는 또 다른 모델(알파 = mean(y), 베타 = 0)을 만들 수 있음.\n",
    "\n",
    "이 경우, 오류를 제곱한 값의 총합은 항상 변화량을 제곱한 값의 총합과 같을 것이며 R 제곱 값은 0이 될 것이다.\n",
    "\n",
    "즉, 오류의 제곱 값을 최소화하는 모델은 항상 mean(y)로 예측하는 것과 별다른 차이가 없다는 것을 의미.\n",
    "\n",
    "최소자승법 모델의 성능은 적어도 평균을 예측하는 모델의 성능만큼 좋아야 함.\n",
    "\n",
    "즉, 오류를 제곱한 값의 총합은 아무리 커봐야 변화량을 제곱한 값의 총합과 동일. 이 경우 R 제곱 값은 0.\n",
    "\n",
    "그리고 오류를 제곱한 값의 총합은 최소한 0이기 때문에 R 제곱 값의 최댓값은 1이다.\n",
    "\n",
    "R 제곱 값이 클수록 모델이 데이터에 더 적합하다는 것을 의미.\n",
    "\n",
    "계산된 R 제곱값은 0.329이기에 만들어진 모델은 어느 정도 적합하나 다른 중요 요소가 존재한다는 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14-2. 경사하강법 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'in_random_order' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5b63c2368092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                  \u001b[0mdaily_minutes_good\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                                  \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                                  0.001)\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-a40e3f1048cc>\u001b[0m in \u001b[0;36mminimize_stochastic\u001b[1;34m(target_fn, gradient_fn, x, y, theta_0, alpha_0)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# and take a gradient step for each of the data points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0min_random_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[0mgradient_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector_subtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalar_multiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'in_random_order' is not defined"
     ]
    }
   ],
   "source": [
    "#이 책 8장의 '걍시히깅법'을 통해 모델 만들기.\n",
    "\n",
    "def squared_error(x_i, y_i, theta):\n",
    "    alpha, beta = theta\n",
    "    return error(alpha, beta, x_i, y_i) ** 2\n",
    "\n",
    "def squared_error_gradient(x_i, y_i, theta):\n",
    "    alpha, beta = theta\n",
    "    return [-2 * error(alpha, beta, x_i, y_i),        #알파에 대한 편미분\n",
    "            -2 * error(alpha, beta, x_i, y_i) * x_i]  #베타에 대한 편미분\n",
    "\n",
    "#임의의 알파, 베타값으로 시작\n",
    "random.seed(0)\n",
    "theta = [random.random(), random.random()]\n",
    "alpha, beta = minimize_stochastic(squared_error,\n",
    "                                 squared_error_gradient,\n",
    "                                 num_friends_good,\n",
    "                                 daily_minutes_good,\n",
    "                                 theta,\n",
    "                                 0.001)\n",
    "print(alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14-3. 최대우도추정법(MLE, Maximum Likelihood Estimation\n",
    " * 최소 자승법을 사용하는 이유: 최대우도추정법을 사용하기 위함이다.\n",
    " * 책의 설명 대신, 아래 링크로 갈음한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "    * 밑바닥부터 시작하는 데이터과학 14장\n",
    "    * https://github.com/EvanOman/Data-Science-from-Scratch/blob/master/Chapter14.ipynb\n",
    "    * https://ratsgo.github.io/statistics/2017/09/23/MLE/\n",
    "    * https://datascienceschool.net/view-notebook/79140e6a9e364bcbb04cb8e525b9dba4/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
