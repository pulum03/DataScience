{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나이브 베이즈(Naive Bayes)\n",
    "\n",
    "나이브는 순진하는 뜻으로, 이유는 데이터셋의 모든 특징들이 동등하고 독립적이라고 가정하기 떄문.\n",
    "\n",
    "나이브 베이즈 분류를 이해하기 위해서는 기본이 되는 베이즈의 정리를 알아야 함.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "조건부 확률 P(A|B)는 사건 B가 발생한 경우 A의 확률.\n",
    "\n",
    "베이즈 정리는 P(A|B)의 추정이 P(A∩B)와 P(B)에 기반을 두어야 한다는 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.1 바보 스팸 필터\n",
    "\n",
    "가능한 모든 메시지에 의의로 메시지를 선택하는 공간이 있다고 가정.\n",
    "\n",
    "메시지가 스팸 : S\n",
    "\n",
    "메시지에 비아그라라는 단어가 포함되는 경우 : V\n",
    "\n",
    "베이즈 정리를 사용하면 메시지에 비아그라라는 단어가 포함되었을 때, 해당 메시지가 스팸일 확률은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(S|V) = {{[P(V|S)P(S)]} \\over {[P(V|S)P(S)+P(V|\\neg S)P(\\neg S)]}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분자는 메시지가 스팸이면서 비아그라 단어를 포함하고 있을 확률을 나타냄.\n",
    "\n",
    "반면, 부모는 메시지가 비아그라라는 단어가 포함되었을 확률.\n",
    "\n",
    "즉, 이 식은 비아그라라는 단어가 포함된 모든 메시지 중에서 스팸 메시지의 비율을 나타낸다고 이해할 수 있다.\n",
    "\n",
    "만약 수많은 스팸 메시지와 스팸이 아닌 메시지를 갖고 있다고, $$P(V|S)$$ $$P(V| \\neg S)$$를 쉽게 추정할 수 있다.\n",
    "만약 메시지가 스팸일 확률과 스팸이 아닌 확률이 동일하다면, 즉,\n",
    "\n",
    "$$ P(S) = P(\\neg S) = 0.5 $$\n",
    "\n",
    "위의 식이 다음과 같이 정리 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(S|V) = {{P(V|S) \\over [P(V|S) + P(V|\\neg S)]}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 스팸 메시지 중 50%, 스팸이 아닌 메시지 중 1%만이 비아그라라는 단어를 포함한다면,\n",
    "\n",
    "비아그라라는 단어를 포함하고 있는 메시지가 스팸일 확률은,\n",
    "\n",
    "0.5/0.5 + 0.01 = 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.2 조금 더 똑똑한 스팸 필터\n",
    "\n",
    "이제 더 많은 단어 w1, ..., wn이 주어졌다고 가정.\n",
    "\n",
    "확률이론을 적용하기 위해 wi가 메시지에 포함되는 경우를 Xi로 나타내고,\n",
    "\n",
    "스팸 메시지에 i번째 단어가 포함되어 있는 확률인 $$P(Xi|S)$$ 와 스팸이 아닌 메시지에 i번째 단어가 포함되어 있는 확률인 $$P(Xi|\\neg S)$$가 주어졌다고 가정.\n",
    "\n",
    "나이브 베이즈(Naive Bayes)의 핵심은 '메시지가 스팸이냐 아니냐가 주어진다는 조건 하에 각 단어의 존재(혹은 부재)는 서로 조건부 독립적이다'라는 (말도 안 되는) 가정에 기반에 둔다.\n",
    "\n",
    "이 가정을 직관적으로 이해하면, 어떤 스팸 메시지가 '비아그라'라는 단어를 포함하고 있다는 점이 같은 메시지가 '롤렉스'라는 단어를 포함하고 있는지를 판단하는 데 도움을 주지 않는다는 것을 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(X1 = x1,...,Xn = xn|S) = P(X1 = x1|S) * ... * P(Xn = xn|S) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'나이브(단순한)' 베이즈라는 이름에서 알 수 있듯, 나이즈 베이즈는 너무 극단적인 가정을 함.\n",
    "\n",
    "예를 들면 사전에 수록된 단어가 '비아그라'와 '롤렉스'뿐이며, 모든 스팸 메시지의 반은 '값싼 비아그라'에 대한 메시지이고, 나머지 스팸 메시지는 '정품 롤렉스'에 대한 메시지라고 가정. 이런 경우 나이브 베이즈는 스팸 메시지에 '비아그라'와 '롤렉스'라는 단어가 포함될 확률을 다음과 같이 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(X1 = 1, X2 = 1|S)  =$$ \n",
    "$$P(X1 = 1|S)P(X2 = 1|S) =$$\n",
    "$$.5*.5 =$$\n",
    "$$ 25 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현실에서는 '비아그라'와 '롤렉스'가 동시에 등장하지 않는다는 가정이 없었기 때문.\n",
    "\n",
    "비록 말도 안되는 가정으로 모델을 만들었지만, 성능은 상당히 뛰어나며, 실제 스팸필터로 사용 가능.\n",
    "\n",
    "'비아그라'라는 단어만으로 스팸을 걸러 내는 필터에서도 사용된 베이즈 정리를 통해 메시지가 스팸일 확률을 다음과 같이 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(S|X = x)  =$$\n",
    "$$ {P(X=x|S) \\over [P(X=s|S)+P(X=x|\\neg S)]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나이브 베이즈 가정에 따르면, 간단하게 각 단어가 메시지에 포함될 확률 값을 모두 곱해서 위 식의 우현에 위치한 값을 모두 구할 수 있음.\n",
    "\n",
    "실제 구현할 땐, 끊임없이 확률 값을 곱하는 것을 피하자. 컴퓨터가 0에 가까운 부동소수점을 제대로 처리하지 못해서 언더플로 문제가 발생하기 때문.\n",
    "\n",
    "기본적인 수학 내용을 더듬으면,\n",
    "\n",
    "log(ab) = log a + log b 이고, exp(log x) = x.\n",
    "\n",
    "그럼 소수점 문제를 피하기 위해 p1 * ... * pn 는 다음과 같이 계산."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\exp(\\log(p1) + ... + \\log(pn)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 스팸이나 스팸이 아닌 메시지에 단어 wi가 포함될 확률인 $$P(Xi|S) 와 P(Xi|\\neg S)$$의 값을 추정하는 문제가 남음.\n",
    "\n",
    "만약 이미 스팸과 스팸이 아닌 메시지로 분류된 '학습'메시지가 충분히 주어졌다면, P(Xi|S)를 추정할 수 있는 가장 간단한 방법은 스팸 메시지 중 wi가 포함되어 있는 메시지의 비율을 사용하는 것.\n",
    "\n",
    "하지만 단점이 있음.\n",
    "\n",
    "주어진 학습 데이터에 '데이터'라는 단어는 스팸이 아닌 메시지에만 포함되어 있다고 상상.\n",
    "\n",
    "즉, P(\"데이터\"|S)=0 일것.\n",
    "\n",
    "그렇다면 나이브 베이즈 분류기는 '데이터'라는 단어가 들어간 메시지를 항상 스팸 메시지가 아니라고 예측 할 것.\n",
    "\n",
    "심지어 '값싼 비아그라와 정품 롤렉스에 대한 데이터'라는 메시지도 스팸이 아니라고 예측할 것.\n",
    "\n",
    "이런 문제를 처리하기 위해서는 일종의 smoothing(평활화)가 필요.\n",
    "\n",
    "Smoothing을 위해 가짜 빈도수(pseudocount) k를 결정하고 스팸 메시지에서 i번째 단어가 나올 확률을 다음과 같이 추정 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(Xi|S) = {(k+wi를 포함하고 있는 스팸 수) \\over (2k + 스팸 수)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(Xi|\\neg S) 도 비슷하게 계산 가능. $$\n",
    "\n",
    "즉, 스팸 메시지에서 i번째 단어가 나올 확률을 계산할 때 해당 단어가 포함된 스팸과 포함되지 않을 스팸이 이미 각각 k번씩 나왔다고 가정.\n",
    "\n",
    "예를 들어 '데이터'라는 단어는 98개의 스팸 문서에서 단 한번도 나타나지 않았지만, k가 1이라면\n",
    "P(\"데이터\"|S)를 1/100 = 0.01로 계산할 수 있다.\n",
    "\n",
    "즉, '데이터'라는 단어가 포함된 메시지가 스팸 메시지일 확률을 0이 아닌 다른 확률값으로 설정할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13-3 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#우선, 메시지를 단어 단위로 잘라 주는 함수를 구현한다.\n",
    "#1. 메시지를 모두 소문자로 바꾼다.\n",
    "#2. re.findall()을 사용해서 문자, 숫자, 혹은 따옴표(apostrophe)가 포함된 모든 단어를 추출한다.\n",
    "#3. 마지막으로, set()을 사용해 중복되는 단어를 제거한다.\n",
    "\n",
    "from typing import Set\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()                         # 소문자로 변환\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text)  # 단어 추출\n",
    "    return set(all_words)                       # 중복되는 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주어진 학습 데이터에서 단어의 빈도수를 세는 함수\n",
    "\n",
    "def count_words(training_set):\n",
    "    \"\"\"학습 데이터는 (메시지 내용, 스팸 여부) 형식으로 구성되어 있다고 한다.\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0]) # counts = [스팸 메시지에서 나온 빈도수(spam_count), \n",
    "                                         # 스팸이 아닌 메시지에서 나온 빈도수(non_spam_count)]\n",
    "                                         # 형태의 list를 값으로 사용하는 dict.\n",
    "    for text, is_spam in training_set:   # 각 단어를 key로 사용함.\n",
    "        for word in tokenize(text):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이제, 평활법(smoothing)으로, 단어의 빈도수로 확률 값을 추정한다.\n",
    "#이 함수는 [단어, 스팸 메시지에서 단어가 나올 확률, 스펨이 아닌 메시지에서 단어가 나올 확률] 형태의 list를 반환한다.\n",
    "\n",
    "def word_probabilities(counts, total_spams, total_non_spams, k= 0.5):\n",
    "    \"\"\"단어의 빈도수를 [ 단어, p(w | 스팸), p(w | !스팸) ] 형태로 리턴함.\"\"\"\n",
    "    return [(w,\n",
    "            (spam + k) / (total_spams + 2 * k),\n",
    "            (non_spam + k) / (total_non_spams + 2 * k))\n",
    "           for w, (spam, non_spam) in counts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#마지막으로, 계산된 단어의 확률과 나이브 베이즈의 기본 가정을 사용해서 메시지가 스팸일 확률을 계산한다.\n",
    "import math\n",
    "\n",
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "    \n",
    "    #모든 단어에 대해 반복.\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "        \n",
    "        # 만약 메시지에 word가 나타나면 해당 단어가 나올 log 확률을 더해 줌.\n",
    "        if word in message_words:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "                \n",
    "        # 만약 메시지에 word가 나타나지 않는다면 해당 단어가 나오지 않을 log 확률을 더해 줌.\n",
    "        # 나오지 않을 확률은 log(1 - 나올 확률) 로 계산.\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "            \n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이브 베이즈 클래스로 모든 함수를 이용한다.\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k = 0.5):\n",
    "        self.k = k  # smoothing factor\n",
    "        self.word_probs = []\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        #스팸 메시지와 스팸이 아닌 메시지의 개수를 세어 줌\n",
    "        num_spams = len([is_spam\n",
    "                         for message, is_spam in training_set\n",
    "                         if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "        \n",
    "        #지금까지 만든 함수에 학습 데이터를 적용\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,\n",
    "                                            num_spams,\n",
    "                                            num_non_spams,\n",
    "                                            self.k)\n",
    "    \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.4 모델 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SpamAssassin public corpus를 활용한 스팸필터기 모델 검증.\n",
    "* 3개의 폴더(spam, easy_ham, hard_ham)가 구성되어 있는데, 각 폴더에는 수많은 메일이 각각의 하나의 파일로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 제목 구분하기: 모든 제목은 'Subject:'라고 시작하므로 그 부분만 찾으면 된다.\n",
    "import glob, re\n",
    "\n",
    "#실제 파일을 저장한 경로로 path를 바꾼다.\n",
    "path = \"../naive_bayes/*/*\"\n",
    "data = []\n",
    "\n",
    "#glob.glob는 주어진 경로에 해당하는 모든 파일 이름을 반환함.\n",
    "for filename in glob.glob(path):\n",
    "    is_spam = \"ham\" not in filename\n",
    "    \n",
    "    with open(filename, errors='ignore') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                #\"Subject: \" 부분을 제거하고 나머지 부분을 유지\n",
    "                #subject = re.sub(r\"^Subject: \", \"\", line).strip()\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                data.append((subject, is_spam))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, prob): #분류기 생성을 위한 데이터 분류 함수\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "#분류기 생성\n",
    "#데이터를 학습 데이터와 평가 데이터로 나누면 분류기를 만들 준비는 끝난다.\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(0)  #예시와 동일한 결과를 얻기 위해서 설정\n",
    "train_data, test_data = split_data(data, 0.75)\n",
    "\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_data)\n",
    "\n",
    "#########################################################################################################\n",
    "#모델의 성능 측정.\n",
    "#[제목, 실제 스팸 여부, 예측된 스팸 확률]\n",
    "classified = [(subject, is_spam, classifier.classify(subject))\n",
    "             for subject, is_spam in test_data]\n",
    "\n",
    "#메시지가 스팸일 확률이 0.5보다 크면 스팸이라고 하자.\n",
    "#그리고, 예측된 스팸 메시지가 실제 스팸인 경우를 세어 보자.\n",
    "counts = Counter((is_spam, spam_probability > 0.5)\n",
    "                 for _, is_spam, spam_probability in classified)\n",
    "\n",
    "print(counts)\n",
    "#TP: 데이터가 실제 스팸이 스팸이라고 예측된 경우(101건)\n",
    "#FP: 데이터가 햄이 스팸으로 예측된 경우(33건)\n",
    "#FN: 데이터가 햄이 햄으로 예측된 경우(704건)\n",
    "#TN: 데이터가 스팸이 햄으로 예측된 경우(38건)\n",
    "#즉, 정밀도(precision) = 101 / (101 + 33) = 75%\n",
    "#    재현율(recall) = 101 / (101 + 38) = 73%\n",
    "#간단한 모델 치고는 나쁘지 않은 성능이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 보기: 가장 잘못 예측된 결과\n",
    "\n",
    "#스팸일 확률을 오름차순으로 정렬\n",
    "classified.sort(key=lambda row: row[2])\n",
    "\n",
    "#스팸이 아닌 메시지 중에서 스팸일 확률이 가장 높은 메시지\n",
    "spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]\n",
    "\n",
    "#스팸 중에서 스팸일 확률이 가장 낮은 메시지\n",
    "hammiest_spams = list(filter(lambda row: row[1], classified))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 보기: 가장 잘못 예측된 결과\n",
    "\n",
    "#스팸일 확률을 오름차순으로 정렬\n",
    "classified.sort(key=lambda row: row[2])\n",
    "\n",
    "#스팸이 아닌 메시지 중에서 스팸일 확률이 가장 높은 메시지\n",
    "spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]\n",
    "\n",
    "#스팸 중에서 스팸일 확률이 가장 낮은 메시지\n",
    "hammiest_spams = list(filter(lambda row: row[1], classified))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#스팸일 확률이 가장 높은 단어\n",
    "def p_spam_given_word(word_prob):\n",
    "    \"\"\"베이즈 정리를 통해 P(S | 메시지가 해당 단어를 포함)을 계산\"\"\"\n",
    "    #word_prob는 word_probabilities 함수에서 계산된 결과\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "words = sorted(classifier.word_probs, key = p_spam_given_word)\n",
    "spammiest_words = words[-5:]\n",
    "hammiest_words = words[:5]\n",
    "\n",
    "print(words[:10])\n",
    "print(spammiest_words[:10])\n",
    "print(hammiest_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류기의 성능을 높이는 방법\n",
    "    * 더 많은 데이터로 분류기를 학습시키기.\n",
    "    * 메시지의 제목뿐만 아니라 내용도 활용하기(단, 메시지의 헤더를 처리할 방안을 세워야 함).\n",
    "    * 지금 만든 분류기는 단어가 학습 데이터에서 단 한 번이라도 나왔다면, 모두 사용함. 단어의 최소 빈도수를 설정함으로써 기준보다 적게 나온 단어를 무시할 수 있도록 분류기를 수정할 수 있음.\n",
    "    * 메시지를 단어 단위로 잘라 줄 때, 동의어를 고려해야 한다. 이를 위해 간단한 stemmer(어간)를 분류기에 추가하여, 비슷한 의미의 단어를 동일한 그룹으로 묶어 준다. 단, 좋은 stemmer를 만드는 것은 어려우니 이미 구현된 Porter stemmer(한국의 KoNLPy 형태소 분석기와 같은, 영어권에서의 분석기) 등을 자주 사용한다.\n",
    "    * 모델의 변수는 메시지에 포함된 모든 단어였지만, 다른 변수를 사용해도 무관하다. 메시지에 숫자가 포함되어 있다면, 숫자를 contains:number 같은 문자열로 대체하도록 tokenizer 함수를 수정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : \n",
    "    * https://gomguard.tistory.com/69\n",
    "    * https://ko.wikipedia.org/wiki/%EB%82%98%EC%9D%B4%EB%B8%8C_%EB%B2%A0%EC%9D%B4%EC%A6%88_%EB%B6%84%EB%A5%98\n",
    "    * 밑바닥부터 시작하는 데이터과학 13장"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
